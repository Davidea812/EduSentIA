# -*- coding: utf-8 -*-
"""Proyecto Final 2IA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10SDBsSI-9z9bJMOSzLNyZ-lZGI_QuZ6b

# **Análisis Inteligente de Evaluaciones Docentes: Detección de Sentimiento, Emociones y Temáticas Clave mediante IA**

## **Introducción**

## **Flujo**

### *Instalación de librerías y configuración inicial*
"""



"""### *Importar librerías necesarias y descargar recursos de nltk para procesamiento*"""
import pickle
import nltk
nltk.download('punkt_tab')
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from unidecode import unidecode
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from transformers import pipeline
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
import umap
import hdbscan
# Descargar recursos de nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""### *Montar Google Drive*

"""

#from google.colab import drive
#drive.mount('/content/drive')

# Cargar archivo JSON (ajusta la ruta al nombre real del archivo)
#df = pd.read_json('/content/drive/MyDrive/Proyecto FInal/comentarios.json')

df = pd.read_json('comentarios.json')
print(df.columns)
df.columns = ['comentario']
# Ver las primeras filas para confirmar
print(df.head())

"""### *Procesamiento de comentarios y análisis de sentimientos y emociones*

Limpieza, lematización y preprocesamiento de texto
"""

# Inicializar herramientas NLP
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Función de limpieza y lematización
def limpiar_comentario(texto):
    texto = str(texto).lower()  # a minúsculas
    texto = unidecode(texto)  # quitar acentos
    texto = re.sub(r'[^\w\s]', '', texto)  # quitar puntuación
    tokens = nltk.word_tokenize(texto)  # tokenización
    tokens = [t for t in tokens if t not in stop_words]  # quitar stopwords
    lemas = [lemmatizer.lemmatize(t) for t in tokens]  # lematización
    return " ".join(lemas)


# Aplicar procesamiento
df['comentario_limpio'] = df['comentario'].apply(limpiar_comentario)

# Mostrar resultado
print(df[['comentario', 'comentario_limpio']].head())

"""Análisis de sentimientos y emociones en comentarios"""

# Inicializar el analizador de sentimientos (VADER)
analyzer = SentimentIntensityAnalyzer()
# Función para obtener el sentimiento (positivo, negativo, neutral)
def obtener_sentimiento(texto):
    puntuacion = analyzer.polarity_scores(texto)
    if puntuacion['compound'] >= 0.05:
        return 'positiveo'
    elif puntuacion['compound'] <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['sentimiento'] = df['comentario_limpio'].apply(obtener_sentimiento)

# DETECCIÓN DE EMOCIONES (RoBERTa)
emotion_model = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=False)
df['emocion'] = df['comentario_limpio'].apply(lambda x: emotion_model(x)[0]['label'])

# COMBINACIÓN: SENTIMIENTO + EMOCIÓN
df['sentimiento_emocion'] = df['sentimiento'] + ' + ' + df['emocion']

# Comentarios de prueba
comentarios_prueba = [
    "I absolutely love this product! It's amazing.",    # Positivo
    "This product is awful. It broke after one use.",   # Negativo
    "This product is okay, nothing special but it works.",  # Neutral
    "Fantastic service! Highly recommend to everyone.",  # Positivo
    "I had high hopes, but this did not meet my expectations.",  # Negativo
    "It works fine, but I’ve seen better alternatives."   # Neutral
]

# Función para procesar y analizar cada comentario
def procesar_comentarios(comentarios):
    resultados = []
    for comentario in comentarios:
        comentario_limpio = limpiar_comentario(comentario)
        sentimiento = obtener_sentimiento(comentario_limpio)
        emocion = emotion_model(comentario_limpio)[0]['label']
        sentimiento_emocion = sentimiento + ' + ' + emocion
        resultados.append({
            'comentario': comentario,
            'comentario_limpio': comentario_limpio,
            'sentimiento': sentimiento,
            'emocion': emocion,
            'sentimiento_emocion': sentimiento_emocion
        })
    return resultados

# Ejecutamos el procesamiento
resultados_prueba = procesar_comentarios(comentarios_prueba)

# Mostrar los resultados
for resultado in resultados_prueba:
    print(f"Comentario: {resultado['comentario']}")
    print(f"Comentario Limpio: {resultado['comentario_limpio']}")
    print(f"Sentimiento: {resultado['sentimiento']}")
    print(f"Emoción: {resultado['emocion']}")
    print(f"Sentimiento + Emoción: {resultado['sentimiento_emocion']}")
    print("-" * 80)

"""# %%

"""

# Eliminar filas vacías y duplicadas
df = df.dropna().drop_duplicates().reset_index(drop=True)

# Asegurarse de que la columna 'comentario' es de tipo cadena y sin espacios innecesarios
df['comentario'] = df['comentario'].astype(str).str.strip()

# Eliminar comentarios triviales
def is_meaningful(text):
    text = text.lower()
    return len(text.split()) > 4 and text not in ['no comentarios', '']

df = df[df['comentario'].apply(is_meaningful)].reset_index(drop=True)

# === 2. OBTENER EMBEDDINGS SEMÁNTICOS ===
model = SentenceTransformer('all-mpnet-base-v2')  # preciso para inglés
embeddings = model.encode(df['comentario'].tolist(), show_progress_bar=True)

# === 3. REDUCCIÓN DE DIMENSIONES CON UMAP ===
umap_5d = umap.UMAP(n_components=5, n_neighbors=15, min_dist=0.0, metric='cosine').fit_transform(embeddings)

# === 4. CLUSTERING CON HDBSCAN ===
clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)
labels = clusterer.fit_predict(umap_5d)
df['cluster'] = labels

# === 5. EXTRACCIÓN DE PALABRAS CLAVE CON KeyBERT ===
kw_model = KeyBERT(model=model)
topics = {}

for label in sorted(set(labels)):
    if label == -1:
        continue  # ruido
    cluster_comentarios = df[df['cluster'] == label]['comentario'].tolist()
    joined_text = " ".join(cluster_comentarios)
    keywords = kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
    topics[label] = [kw[0] for kw in keywords]

# Guardar embeddings, clusters y palabras clave
with open('comentarios_procesados.pkl', 'wb') as f:
    pickle.dump({'embeddings': embeddings, 'labels': labels, 'topics': topics}, f)

# === 6. IMPRIMIR TEMAS DETECTADOS ===
print("=== Temas detectados ===\n")
for label, words in topics.items():
    count = (df['cluster'] == label).sum()
    print(f"Cluster {label} ({count} comentarios): {', '.join(words)}")

# === 8. VISUALIZACIÓN 2D UMAP ===
umap_2d = umap.UMAP(n_components=2, n_neighbors=15, metric='cosine').fit_transform(embeddings)
df['x'] = umap_2d[:, 0]
df['y'] = umap_2d[:, 1]

plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x='x', y='y', hue='cluster', palette='tab10', legend='full', s=50)
plt.title("Distribución de comentarios por temática (clusters)")
plt.xlabel("UMAP Dim 1")
plt.ylabel("UMAP Dim 2")
plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
